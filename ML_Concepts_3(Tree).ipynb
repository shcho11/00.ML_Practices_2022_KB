{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ML이론3(Tree).ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyP+iKnMaGKU8gAiQD4EezEq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shcho11/00.ML_Practices_2022_KB/blob/main/ML%EC%9D%B4%EB%A1%A03(Tree).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 이론 내용 출처 : **2022-1 KB ACE Academy DT개발 STEP2 과정**"
      ],
      "metadata": {
        "id": "qdbEDyDgx_wi"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rW0EBV-tV1Xh"
      },
      "source": [
        "## Decision Tree(CART : Classification and Regression Tree)\n",
        "> **`Decision Tree`** 모델은 **예측/분류가 모두 가능**한 **지도학습** 머신러닝 모델이다.   \n",
        "스무고개 게임을 하듯 여러 개의 가정을 데이터에 반영하고 이를 바탕으로 결정경계(decision boundary)를 생성  \n",
        "모델 예측 및 분류 결과에 따른 해석이 굉장히 용이하여 **모델 해석이 필요한 문제에 사용**한다.ex)신용평가, 모델분류  \n",
        "최근에는 `Decision Tree`모델을 베이스로 한 부스팅 트리 모델(**`Xgboost`**, **`LightGBM`**, **`Catboost`**)등으로 데이터분석 대회 수상을 하면서 실무 적용 케이스가 많아졌다.\n",
        "\n",
        "### 모델구조\n",
        "> 뿌리 노드(root node) : 최상위 노드, 모든 샘플 포함  \n",
        "잎 노드(leaf node) : 최하위 노드, 여기에 속한 샘플이 어떤 클래스인지 결정 됨  \n",
        "노드(node) : 뿌리 노드와 잎 노드 사이에 있는 노드  \n",
        "가지(branch) : 노드를 나누는 기준  \n",
        "깊이(depth) : 뿌리 노드와 잎 노드 까지의 노드 갯수\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=1gTRvBWaKpbR5VI9Iv1_OlwKg4YQGPxXr\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0CByJxzyV1Xi"
      },
      "source": [
        "### 모델학습\n",
        "#### 불순도\n",
        "> `Decision Tree` 모델을 학습시키는 방법  \n",
        "정보화 이론에서 사용하는 Gini 계수와 엔트로피를 사용한다.  \n",
        "불순도가 0.5에 가까수록 불순도가 높고 0에 가까울 수록 순도가 높다.  \n",
        "즉, 한 노드의 불순도가 가능한 많이 떨어지도록(순도가 올라가도록) 노드를 나눈다.\n",
        "\n",
        "$$ Gini = 1 - \\sum_1^n{(p_i)^2} $$\n",
        "\n",
        "$$ Entropy = - \\sum_1^n{p_iln(p_i)} $$\n",
        "\n",
        "#### Gini index\n",
        "위 예시에서 뿌리 노드 기준 지니계수 계산법  \n",
        "class1 : 삼각형  \n",
        "class2 : 동그라미  \n",
        ">X < 0\n",
        ">> True = class1 3개, class2 4개  \n",
        "$1 - ({3 \\over 3+4})^2 - ({4 \\over 3+4})^2 = 0.48$  \n",
        "False = class1 4개, class2 3개  \n",
        "$1 - ({4 \\over 4+3})^2 - ({3 \\over 4+3})^2 = 0.48$  \n",
        "total Gini 계수  \n",
        "$1 - ({7 \\over 7+7})0.48 - ({7 \\over 7+7})0.48 = 0.52$\n",
        "\n",
        "위 예시에서 잎 노드 기준 지니계수 계산법  \n",
        "class1 : 삼각형  \n",
        "class2 : 동그라미  \n",
        ">Y < 1\n",
        ">> True = class1 3개, class2 0개  \n",
        "$1 - ({3 \\over 3})^2 - ({0 \\over 3})^2 = 0$  \n",
        "False = class1 0개, class2 4개  \n",
        "$1 - ({0 \\over 4})^2 - ({4 \\over 4})^2 = 0$  \n",
        "total Gini 계수  \n",
        "$1 - ({4 \\over 4+3})0 - ({3 \\over 4+3})0 = 0$\n",
        "\n",
        "위의 예시에서는 최적화 과정을 거치지 않은 결정경계를 생성했지만 실제 알고리즘은 각 분지 기준에 대한 학습을 진행합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jf2L4kuNV1Xq"
      },
      "source": [
        "### 가지치기 (pruning)\n",
        ">`Decision Tree`모델은 모든 **잎 노드의 불순도가 0이 되는 순간까지 모델을 성장**시키면서 크기를 키워나간다.  \n",
        "순수 노드로만 이루어진 트리 모델은 훈련 데이터를 100% 정확도로 맞출 수 있다.  \n",
        "이러한 특성 때문에 트리 모델은 **과적합에 취약**하다.  \n",
        "과적합 방지를 위해서는 **트리의 복잡도를 제어** 할 필요가 있다.\n",
        "\n",
        ">과적합 방지를 위한 모델링 파라메터  \n",
        ">> - **`max_depth`** : 트리의 최대 깊이  \n",
        "- `max_leaf_nodes` : 잎 노드의 최대개수  \n",
        "- `min_sample_leaf` : 잎 노드가 되기 위한 최소 샘플 갯수  \n",
        "- `min_sample_split` : 잎 노드가 분지 되기 위한 최소 샘플 갯수\n",
        "\n",
        "위의 iris 데이터는 3개의 클래스로 이루어진 데이터셋이지만 모델플로팅 결과 2뎁스의 노드에서 어느정도 데이터 구분이 되었습니다.  \n",
        "이를 기준으로 사후 가지치기를 진행 해 보겠습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k13YFxyTt0YB"
      },
      "source": [
        "## Decision tree regressor\n",
        "> `Decision Tree`모델은 알고리즘 특성으로 분류 및 예측 모델링에 모두 사용이 가능하다.  \n",
        "일반적으로 잎 노드에 속한 학습샘플의 값의 평균을 바탕으로 예측값을 결정한다.  \n",
        "회귀모델 평가 방법인 MSE를 각 노드에 속한 샘플에 적용하고 이를 최소화 시킨다.  \n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=1VNT8MulVBhqDLj0tVTRrf8s-8EIv5RZP\">\n",
        "<img src=\"https://drive.google.com/uc?id=1ICIKUdPHbx9ZkBSKzOgl7248QaIQrq2U\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-07-23T06:30:12.192008Z",
          "start_time": "2021-07-23T06:30:12.189331Z"
        },
        "id": "ZllsM81St0YD"
      },
      "source": [
        "## Random Forest\n",
        ">**`Random forest`** 는 **`Decision Tree`** 모델의 **모형 결합(ensemble)방법론**  \n",
        "\n",
        "### ensemble(앙상블)\n",
        "> **복수의 예측 모형을 결합**하여 더 나은 성능의 예측을 하려는 시도이다.  \n",
        "단일 모형을 사용할 때 보다 **성능 분산이 감소**하고, 즉 **과적합을 방지**한다.  \n",
        "개별 모형이 성능이 안좋을 경우에는 결합 모형의 성능이 더 향상된다.  \n",
        "앙상블 방법론에는 **배깅**, **부스팅**, **스태킹**이 있다.\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=1JJVUyYwHD2ddpigy0D3mG5KFLc5Yq1qR\">\n",
        "\n",
        "### bagging(배깅) : Bootstrap Aggregating\n",
        "> **복원 추출**로 여러개의 sub sample 데이터셋을 만든다.  \n",
        "각각의 데이터셋을 개별 모델에 학습시켜 서로 다른 결과를 얻는다.  \n",
        "투표법 혹은 평균법을 사용하여 개별 모델 결과를 바탕으로 최종 추정치를 얻는다.  \n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=1CMsFhLTApJqrOlGdqVE4qWH89j04w_xF\">\n",
        "\n",
        "### Random Forest bootstrap\n",
        "> 복원 추출 된 sub sample 에서 랜덤으로 feature를 선택하여 모델 학습에 사용한다.\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=1MfkxFwIsQJgjT_VD_CFl_fM3WJfc_3BG\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XFh2Ok8St0YD"
      },
      "source": [
        "### 과적합 방지를 위한 모델링 파라메터  \n",
        "> - **n_estimators** : 사용 할 트리 모델 갯수  \n",
        "- **max_depth** : 트리의 최대 깊이"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zpsr8J0Yt0YE"
      },
      "source": [
        "## Boosting Tree\n",
        "> 배깅과 부스팅의 차이점은 학습을 위해 사용하는 개별모델을 병렬/직렬로 구성함에 있다.  \n",
        "배깅의 경우 sub sample에 따라 개별 모델을 모두 학습시키고 결과를 투표 혹은 평균을 내어 예측한다면  \n",
        "부스팅은 **개별 모델의 학습을 순차적**으로 시키며 이전 개별 모델의 결과 중 **오분류 된 데이터 혹은 오차에 가중치 부여**  \n",
        "초기에는 동일 가중치를 갖지만 각 학습 과정을 거치며 복원 추출 시 가중치의 분포/이전 round의 오차를 고려  \n",
        "\n",
        ">> 해당모델에는 `Adaboost`, `GBM`, `Xgboost`, `lightGBM`, `catboost`가 있다.\n",
        "\n",
        "### bagging 과 boosting\n",
        "<img src=\"https://drive.google.com/uc?id=1rhB9YkRkKILRb0GqOfa99K-EQ7CEcrPc\">\n",
        "\n",
        "### Adaptive booting(Adaboost)\n",
        "> a -> f 순서로 학습이 진행 되고 있다. 각 학습 단계(round)에서 오분류 된 데이터에 가중치를 부여하고  \n",
        "다음 라운드에서 가중치가 부여 된 데이터를 잘 맞추기 위한 개별모델이 학습 된다.  \n",
        "최종 모델은 개별 모델의 결과가 합쳐져서 최종 모델링이 된다.\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=1VKmbttZT4aPOAaYuqhw2os7nFMOs6MMJ\" width=700>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PTMGhzxQt0YE"
      },
      "source": [
        "### gradient boost\n",
        "이전 round 모델의 데이터별 오류를 학습하는 모델을 사용하여 점진적으로 총 모델링 오차를 줄이는 부스팅 방법\n",
        "\n",
        "$$y = h_0(x) + error_0 $$\n",
        "$$error_0 = h_1(x) + error_1 $$\n",
        "$$error_1 = h_2(x) + error_2 $$\n",
        "$$\\vdots$$\n",
        "$$y = h_0(x) + h_1(x) + h_2(x) + \\cdots + small error $$\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=1Ty-CEINGgXuHtghd0GuDsxO1CfWahDYB\" width=700>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZpZ3YPxdt0YE"
      },
      "source": [
        "### xgboost\n",
        "> 머신러닝 알고리즘 대회인 kaggle, KDD cup등에서 우승을 한 팀들이 xgboost를 많이 활용한 것이 알려지면서 주목받음.  \n",
        "boosting 모델에서 오류를 학습하여 다음 round에 반영시키는 것은 gadient boosting과 큰 차이가 없음.  \n",
        "다만, 학습을 위한 비용함수에 규제화 식이 추가되어 모델이 과적합 되는 것을 방지함.  \n",
        "규제화를 통해 복잡한 모델에 패널티를 부여  \n",
        "\n",
        "$$obj^{(t)} = \\sum_1^{n} l(y_i, \\hat{y}_i^{(t)}) + \\sum_{i=1}^t \\Omega(f_i) $$\n"
      ]
    }
  ]
}
